{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2fb1063",
   "metadata": {},
   "source": [
    "# Analyze the gamma rule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16592181",
   "metadata": {},
   "source": [
    "## Defining the Surrogate model for the gamma rule\n",
    "\n",
    "### Linear layer\n",
    "\n",
    "We look at the simple case of a linear model, that shares its parameters in every layer. \n",
    "We have a matrix $A^{(t)} \\in \\mathbb{R}^{n \\times n}$ and define\n",
    "\n",
    "$$ h^{(0)} = x_0 $$\n",
    "$$ h^{(t)} = A^{(t)} h^{(t-1)} $$\n",
    "\n",
    "for some initialization $x_0 \\in \\mathbb{R}^{n}$.\n",
    "Instead of the intuitive surrogate model with $R^+_{\\cdot | \\cdot}(t) = A^{(t)}$,\n",
    "we would now like to define an alternative surrogate model, that constructs the same layerwise activations $ R^+_{\\cdot}(t) $ but with different transition coefficients $R^+_{j | i}(t)$.\n",
    "\n",
    "**Forward surrogate model.** \n",
    "We choose a $\\gamma \\in \\mathbb{R}^+$, and abbreviate $w_{j, i} = A^{(t)}_{j, i}$ for the current layer $t$.\n",
    "$w_{j | i}^+$ is the positive part of the weights $w_{j | i}$.\n",
    "\n",
    "$$ R^+(0) = x_0 $$\n",
    "$$ R^+_{j | i}(t) = (w_{j | i} + \\gamma \\cdot w_{j | i}^+) \\cdot c_j $$\n",
    "$$ c_j = \\frac{ \\sum_i R^+_{i}(t-1) w_{j | i} }{ \\sum_i R^+_{i}(t-1) (w_{j | i} + \\gamma \\cdot w_{j | i}^+) } $$\n",
    "\n",
    "where $c_j \\in [0, 1]$ is a scaling factor per output neuron $j$.\n",
    "\n",
    "**Surrogate model check** We want to check, how good the surrogate model recovers the original mode, we want to show that $R^+(t) = h(t)$.\n",
    "\n",
    "*Proof.* We show by induction, starting with $t = 0$. By definition,\n",
    "\n",
    "$$ R^+(0) = x_0 = h^{(0)} $$\n",
    "\n",
    "Now, in the induction step $t \\rightarrow t+1$, we assume $ R^+(t-1) = h^{(t-1)}$, then\n",
    "\n",
    "\\begin{align*}\n",
    "R^+_{j}(t) \n",
    "&= \\sum_i R^+_{j | i}(t) \\cdot R^+_{i}(t-1) \\\\\n",
    "&= \\sum_i R^+_{i}(t-1) \\cdot (w_{j | i} + \\gamma \\cdot w_{j | i}^+) \\cdot c_j \\\\\n",
    "&= c_j  \\cdot \\sum_i R^+_{i}(t-1) \\cdot (w_{j | i} + \\gamma \\cdot w_{j | i}^+) \\\\\n",
    "&= \\frac{ \\sum_i R^+_{i}(t-1) w_{j | i} }{ \\sum_i R^+_{i}(t-1) (w_{j | i} + \\gamma \\cdot w_{j | i}^+) }\n",
    "         \\cdot \\sum_i R^+_{i}(t-1) \\cdot (w_{j | i} + \\gamma \\cdot w_{j | i}^+) \\\\\n",
    "&= \\sum_i R^+_{i}(t-1) w_{j | i} \\\\\n",
    "&= \\sum_i h^{(t-1)} w_{j | i} \\\\\n",
    "&= h^{(t)} \\\\\n",
    "\\end{align*}\n",
    "\n",
    "**Backward surrogate model.**\n",
    "For the backward chain we have \n",
    "\n",
    "\\begin{align*}\n",
    "R^-_{i}(t-1) \n",
    "&= \\sum_j \\frac{ R^+_{j | i}(t) \\cdot R^+_{i}(t-1) }\n",
    "                { \\sum_{i'} R^+_{j | i'}(t) \\cdot R^+_{i'}(t-1)} \n",
    "            R^-_{j}(t) \\\\\n",
    "&= \\sum_j \\frac{ (w_{j | i} + \\gamma \\cdot w_{j | i}^+) \\cdot c_j \\cdot h_{i}^{(t-1)} }\n",
    "                { \\sum_{i'} (w_{j | i'} + \\gamma \\cdot w_{j | i'}^+) \\cdot c_j \\cdot h_{i'}^{(t-1)} } \n",
    "            R^-_{j}(t) \\\\\n",
    "&= \\sum_j \\frac{ (w_{j | i} + \\gamma \\cdot w_{j | i}^+) \\cdot h_{i}^{(t-1)} }\n",
    "                { \\sum_{i'} (w_{j | i'} + \\gamma \\cdot w_{j | i'}^+) \\cdot h_{i'}^{(t-1)} } \n",
    "            R^-_{j}(t) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "which is known as the LRP-$\\gamma$ rule."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "0494f27bb39db6b3d215c7cce3d65be35d8071cb380b71d0665c33206dc0f891"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
